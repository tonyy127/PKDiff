# PKDiff-for-multimodal-Remote-Sensing-Segmentation
The code is currently being organized and supplemented.


ğŸ›°ï¸ å¤šæ¨¡æ€é¥æ„Ÿåˆ†å‰²ï¼ˆDDPï¼‰è®­ç»ƒ

åŸºäº PyTorch DistributedDataParallel (DDP) çš„å¤šå¡è®­ç»ƒä¸è¯„æµ‹è„šæœ¬ï¼šæ”¯æŒ å•æœºå¤šå¡ / å¤šæœºå¤šå¡ã€Linear Warmup â†’ Cosine å­¦ä¹ ç‡è°ƒåº¦ã€æŒ‰å‚æ•°ååˆ†ç»„ LRã€åˆ†å¸ƒå¼è¯„æµ‹èšåˆä¸ NVML æ˜¾å¡ä¿¡æ¯æ‰“å°ã€‚èƒ½è·‘ï¼Œå¤šå¡å¿«ã€‚

1) é¡¹ç›®ç®€ä»‹ï¼ˆåˆ†å¸ƒå¼è®­ç»ƒï¼‰

é‡‡ç”¨ torch.distributed çš„ NCCL åç«¯ï¼›torchrun å¯åŠ¨ï¼Œæ¯è¿›ç¨‹ç»‘ä¸€å¼  GPUï¼ˆè¯» RANK/WORLD_SIZE/LOCAL_RANKï¼‰ã€‚

è®­ç»ƒé˜¶æ®µï¼šæŒ‰æ­¥å­¦ä¹ ç‡ï¼ˆLinear warmup â†’ Cosineï¼‰ï¼›è¯„æµ‹é˜¶æ®µï¼šå„ rank é¢„æµ‹å‘é‡ all_gather åˆ° rank0 ç»Ÿä¸€è®¡ç®—æŒ‡æ ‡å¹¶å¹¿æ’­ã€‚

ä»£ç ç»“æ„ï¼ˆå»ºè®®ï¼‰ï¼š

repo_root/
â”œâ”€ train.py
â”œâ”€ networks/network.py            # NetworkConfig, Network
â”œâ”€ trainer.py                     # Trainer, TrainerConfig
â”œâ”€ utils1.py                      # æ•°æ®è·¯å¾„/å¸¸é‡ï¼ˆtrain_ids/test_ids/BATCH_SIZE/...ï¼‰
â”œâ”€ utils/evaluation.py            # Evaluator, denoise, metrics, accuracy
â”œâ”€ eval_patch_dataset.py          # EvalPatchDataset
â””â”€ requirements.txt

2) åˆ›å»ºç¯å¢ƒï¼ˆæ¨èï¼šconda + pipï¼‰

rasterio/GDAL æ¯”è¾ƒâ€œé‡â€ï¼Œå…ˆç”¨ conda-forge è£…å®ƒä»¬ï¼Œå‰©ä¸‹çš„èµ° pipã€‚

# â‘  æ–°å»º & æ¿€æ´»ç¯å¢ƒ
conda create -n rsddp python=3.10 -y
conda activate rsddp

# â‘¡ å…ˆè£…é‡ä¾èµ–ï¼ˆå¸¦å¥½äºŒè¿›åˆ¶ï¼‰
conda install -c conda-forge rasterio affine -y

3) å®‰è£… requirements.txt ä¾èµ–

é¡¹ç›®æ ¹ç›®å½•æ‰§è¡Œï¼š

pip install -r requirements.txt


è¯´æ˜ï¼šrequirements.txt é¡¶éƒ¨åŒ…å«
--extra-index-url https://download.pytorch.org/whl/cu121
å¯¹åº” CUDA 12.1 çš„ PyTorch è½®å­ã€‚å¦‚æœä½ æ˜¯ CPU-only æˆ–å…¶å®ƒ CUDA ç‰ˆæœ¬ï¼Œè¯·åˆ é™¤/ä¿®æ”¹è¯¥è¡Œå¹¶ç›¸åº”è°ƒæ•´ torch/torchvision ç‰ˆæœ¬æ ‡è®°ï¼ˆå¦‚å»æ‰ +cu121ï¼‰ã€‚

4) å¼€å§‹è®­ç»ƒ
å•æœºå¤šå¡ï¼ˆä»¥ 4 å¡ä¸ºä¾‹ï¼‰
CUDA_VISIBLE_DEVICES=0,1,2,3 \
torchrun --nproc_per_node=4 train.py

å¤šæœºå¤šå¡ï¼ˆç¤ºä¾‹ï¼š2 æœº Ã— æ¯æœº 4 å¡ï¼‰

åœ¨ node0ï¼ˆmasterï¼‰ï¼š

MASTER_ADDR=node0.host
MASTER_PORT=29500

torchrun --nnodes=2 --node_rank=0 \
         --nproc_per_node=4 \
         --master_addr=$MASTER_ADDR --master_port=$MASTER_PORT \
         train.py


åœ¨ node1ï¼š

MASTER_ADDR=node0.host
MASTER_PORT=29500

torchrun --nnodes=2 --node_rank=1 \
         --nproc_per_node=4 \
         --master_addr=$MASTER_ADDR --master_port=$MASTER_PORT \
         train.py


æ²¡æœ‰ InfiniBand æ—¶å¯åŠ ï¼šexport NCCL_IB_DISABLE=1ï¼ˆæ›´çœäº‹ï¼‰ã€‚
